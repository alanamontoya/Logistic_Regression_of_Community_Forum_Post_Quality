{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf995fe",
   "metadata": {},
   "source": [
    "_Import the `pandas`, `numpy`, `train_test_split`, `LogisticRegression`, and `accuracy_score`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb909c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b2718",
   "metadata": {},
   "source": [
    "## Problem:\n",
    "An automated answer-rating site marks each post in a community forum website as “good” or “bad” based on the quality of the post. The `quality` CSV file, contains the various types of quality as measured by the tool. Following are the type of qualities that the dataset contains:\n",
    "\n",
    "i. `num_words`: number of words in the post\n",
    "\n",
    "ii. `num_characters`: number of characters in the post\n",
    "\n",
    "iii. `num_misspelled`: number of misspelled words\n",
    "\n",
    "iv. `bin_end_qmark`: if the post ends with a question mark\n",
    "\n",
    "v. `num_interrogative`: number of interrogative words in the post\n",
    "\n",
    "vi. `bin_start_small`: if the answer starts with a lowercase letter. (‘1’ means yes, otherwise no)\n",
    "\n",
    "vii. `num_sentences`: number of sentences per post\n",
    "\n",
    "viii. `num_punctuations`: number of punctuation symbols in the post\n",
    "\n",
    "ix. `label`: the label of the post (‘G’ for good and ‘B’ for bad) as determined by the tool.\n",
    "\n",
    "Create a logistic regression model to predict the class label from the first eight attributes of the question set. Then try doing the same using two different subsets (your choice) of those eight attributes. Report the accuracies of each of these three models. For the two subsets that you use, provide some justification (why you chose those features in a given subset). As we discussed in the class, it's useful to report not just a single accuracy number for a given model, but either an average accuracy over many runs or a distribution of accuracies over those runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c6db5",
   "metadata": {},
   "source": [
    "_First we need to load in the `quality` dataset:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1729d8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality = pd.read_csv('quality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938319d",
   "metadata": {},
   "source": [
    "_Prior to building our models, we can change `label` (indicating whether a post was good (`G`) or bad (`B`)) from a string categorical type to a number categorical type. This allows us to transform this variable to be used as a binomial distribution which can then be modeled using logistic regression. We can assign them as follows:_\n",
    "> _`G` (good)   :   0_\n",
    "\n",
    "> _`B`  (bad)   :   1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d202d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality[\"label\"] = np.where(df_quality[\"label\"] ==  \"B\", 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf347db",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### _Create a logistic regression model to predict the class label from the first eight attributes_\n",
    "\n",
    "_First we can seperate the variables into features and label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7d9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = df_quality.drop(\"S.No.\", axis = 1).drop(\"label\", axis = 1)\n",
    "label = df_quality[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28567556",
   "metadata": {},
   "source": [
    "_Next we can split the data into 70% training and 30% testing. Note that `X` represents the features and `y` represents the label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56261a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(features1, label, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13687efd",
   "metadata": {},
   "source": [
    "_Now we can fit a logistic regression model using `X_train1` as the training features and `y_train1` as the training label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f84ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression(max_iter = 10000)\n",
    "model1 = logit_model.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e07ce",
   "metadata": {},
   "source": [
    "_Then we can use this trained model to make predictions for `label` using `X_test1` as the test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4784744",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db20ca",
   "metadata": {},
   "source": [
    "_Then we can use these predictions (`pred1`) and compare them against the actual values (`y_test1`) to determine the accuracy of this model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130ddd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test1, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a9b03",
   "metadata": {},
   "source": [
    "_However, it is more useful to get an average accuracy. So, we can build a function to do this. This function will take in as an input `n` number of times to iterate through the process (spliting the data, training the model, making predictions, and finding the accuracy), `features` to represent which variables to be included as features, and `labels` to indicate which variable to be included as the label. Having the second two as inputs allows this function to become more versitile so that we can use it for the next two parts._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fffcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_accuracy(n, features, labels):\n",
    "    tot = 0\n",
    "    for i in range(0, n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.30)\n",
    "        logit_model.fit(X_train, y_train)\n",
    "        pred = logit_model.predict(X_test)\n",
    "        tot = tot + accuracy_score(y_test, pred)\n",
    "    return(tot/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190efa8",
   "metadata": {},
   "source": [
    "_Using this function, we can then find the average accuracy of the model for 100 iterations given the features and label indicated above (`features1` and `label`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a29102b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6455555555555555"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(100, features1, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71a8beb",
   "metadata": {},
   "source": [
    "_Hence, the number above represents the average accuracy of the logistic regression model that predicts the class label from the first eight attributes._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52051ef1",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### _Create a logistic regression model to predict the class label from the spelling, captilization, and grammar,  attributes_\n",
    "\n",
    "_When looking at the eight attributes, I noticed that they can be categorized based on different aspects. One such category that can be made could include `num_misspelled`, `bin_start_small`, and `num_punctuations`. These variables all relatively relate to how well a user knows these basic English structures or whether they cared to include them. Hence, these three variables may be useful to model the quality of a post._\n",
    "\n",
    "_First we can assign these three variables as features. Since the label will be the same as above, we do not need to recreate this variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d2ae53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = df_quality.drop(\"S.No.\", axis = 1).drop(\"num_words\", axis = 1).drop(\"num_characters\", axis = 1).drop(\"bin_end_qmark\", axis = 1).drop(\"num_interrogative\", axis = 1).drop(\"num_sentences\", axis = 1).drop(\"label\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace800cb",
   "metadata": {},
   "source": [
    "_Next we can split the data into 70% training and 30% testing. Note that `X` represents the features and `y` represents the label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f810f89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features2, label, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60201b9",
   "metadata": {},
   "source": [
    "_Now we can fit a logistic regression model using `X_train2` as the training features and `y_train2` as the training label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c37286",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = logit_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5db129",
   "metadata": {},
   "source": [
    "_Then we can use this trained model to make predictions for `label` using `X_test2` as the test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b64e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = model2.predict(X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6e815",
   "metadata": {},
   "source": [
    "_Then we can use these predictions (`pred2`) and compare them against the actual values (`y_test2`) to determine the accuracy of this model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43fa105c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test2, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec6ae0",
   "metadata": {},
   "source": [
    "_Again, since it is more useful to get an average accuracy of this model, we can use the `get_avg_accuracy` function we created previously to find the average accuracy of the model for 100 iterations given the features and label indicated above (`features2` and `label`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff402bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4388888888888887"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(100, features2, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46764f34",
   "metadata": {},
   "source": [
    "_From this average accuracy, we can see that the first model made from all eight attributes performed better than this second model. This means that these three attributes (`num_misspelled`, `bin_start_small`, and `num_punctuations`) are not very good at predicting the quality of posts (when modeled together)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967f2ee",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### _Create a logistic regression model to predict the class label from attributes implying a question_\n",
    "\n",
    "_Another category that could be made from the eight attribites is those that pertain to questions. Two variables that relate to this are `bin_end_qmark` and `num_interrogative`. Building a model based on these attributes and then measuring its accuracy could examine how well the automated answer-rating site predicts the quality of posts when questions are included._\n",
    "\n",
    "_First we can assign these three variables as features. Since label will be the same as above, we do not need to recreate this variable._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea9db12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = df_quality.drop(\"S.No.\", axis = 1).drop(\"num_words\", axis = 1).drop(\"num_characters\", axis = 1).drop(\"num_misspelled\", axis = 1).drop(\"bin_start_small\", axis = 1).drop(\"num_sentences\", axis = 1).drop(\"num_punctuations\", axis = 1).drop(\"label\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aecaec1",
   "metadata": {},
   "source": [
    "_Next we can split the data into 70% training and 30% testing. Note that `X` represents the features and `y` represents the label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "811182e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(features3, label, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00586a",
   "metadata": {},
   "source": [
    "_Now we can fit a logistic regression model using `X_train3` as the training features and `y_train3` as the training label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1719bfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = logit_model.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0300a3",
   "metadata": {},
   "source": [
    "_Then we can use this trained model to make predictions for label using `X_test3` as the test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "865a9427",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = model3.predict(X_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c6f24",
   "metadata": {},
   "source": [
    "_Then we can use these predictions (`pred3`) and compare them against the actual values (`y_test3`) to determine the accuracy of this model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7cbfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test3, pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82374cf",
   "metadata": {},
   "source": [
    "_Again, since it is more useful to get an average accuracy of this model, we can use the `get_avg_accuracy` function we created previously to find the average accuracy of the model for 100 iterations given the features and label indicated above (`features3` and `label`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7ca7a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42777777777777787"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_avg_accuracy(100, features3, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401cedbc",
   "metadata": {},
   "source": [
    "_From this average accuracy, we can see that the first model made from all eight attributes performed better than this third model and that the second and third model perform about the same. This means that these two attributes (`bin_end_qmark` and `num_interrogative`) are not very good at predicting the quality of posts (when modeled together)._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
